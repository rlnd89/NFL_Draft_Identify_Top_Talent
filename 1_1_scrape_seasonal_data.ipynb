{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to pull seasonal data (2000-2020) from the following https://www.sports-reference.com/ websites:\n",
    "    - College Football: https://www.sports-reference.com/cfb/\n",
    "    - NFL: https://www.pro-football-reference.com/\n",
    "    \n",
    "Tables/sites to be scripted:\n",
    "    - NCAA team Ratings: https://www.sports-reference.com/cfb/years/yyyy-ratings.html\n",
    "    - NCAA Team Offense: https://www.sports-reference.com/cfb/years/yyyy-team-offense.html\n",
    "    - NCAA Team Defense: https://www.sports-reference.com/cfb/years/yyyy-team-defense.html\n",
    "    - NCAA Player Passing Stats: https://www.sports-reference.com/cfb/years/yyyy-passing.html\n",
    "    - NCAA Player Receiving Stats: https://www.sports-reference.com/cfb/years/yyyy-receiving.html\n",
    "    - NCAA Player Rushing Stats: https://www.sports-reference.com/cfb/years/yyyy-rushing.html\n",
    "    - NCAA Player Kicking Stats: https://www.sports-reference.com/cfb/years/yyyy-kicking.html\n",
    "    - NCAA Player Punting Stats: https://www.sports-reference.com/cfb/years/yyyy-punting.html\n",
    "    - NCAA Season Summary: https://www.sports-reference.com/cfb/years/yyyy.html\n",
    "    - NFL Combine Results: https://www.pro-football-reference.com/draft/yyyy-combine.htm\n",
    "    - NFL Draft Results: https://www.pro-football-reference.com/years/yyyy/draft.htm\n",
    "    - NFL Standings and Team Stats: https://www.pro-football-reference.com/years/yyyy/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                   # to make html requests\n",
    "from bs4 import BeautifulSoup     # to pull data from html websites\n",
    "import pandas as pd               # to create dataframes and concat dataframes and read html tables\n",
    "import time                       # to set delays between requests\n",
    "import numpy as np                # to choose random elements from list\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000-2020 - e.g 2000 nfl draft is based on 1999 college season\n",
    "years_nfl = ['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019', '2020']\n",
    "\n",
    "# 1999-2019 - e.g 1999 season ends in 2000 and players get drafted in 2000\n",
    "years_ncaa = ['1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019']\n",
    "\n",
    "# list of tables for player stats\n",
    "player_tables = ['passing','rushing','receiving','kicking','punting']\n",
    "\n",
    "# list of delays to put between get requests\n",
    "delays = [5, 6, 7, 8, 9, 10, 14, 17, 19, 23, 27, 29, 31, 33, 35, 42, 45]\n",
    "\n",
    "# set request headers based on website\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'referer': 'http://www.google.com/',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Define functions to pull data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Pull combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get combine data\n",
    "def pullCombineData(years):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.pro-football-reference.com/draft/' + year + '-combine.htm'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with headers\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get combine table\n",
    "            table = soup.find('table', {'id': 'combine'})\n",
    "            # get column headers\n",
    "            column_headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True) \n",
    "\n",
    "            # get player ids and ncaa links, if not exist put N/A\n",
    "            player_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for th in tr.find_all('th'):\n",
    "                        if th.text not in column_headers:\n",
    "                            try:\n",
    "                                player_ids.append(th['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ids.append('N/A')\n",
    "                    for td in tr.find_all('td'):\n",
    "                            if td['data-stat'] == \"college\":\n",
    "                                if td.find_all('a'):\n",
    "                                    for a in td.find_all('a'):\n",
    "                                        ncaa_links.append(a['href'])\n",
    "                                else:\n",
    "                                    ncaa_links.append('N/A')\n",
    "            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_ID\", player_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    #  in one DataFrame\n",
    "    combines_df = pd.concat(dfs, ignore_index=True)\n",
    "    combines_df.to_csv('nfl_combine_2000_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullCombineData(years_nfl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Pull draft results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get draft data\n",
    "def pullDraftData(years):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.pro-football-reference.com/years/' + year + '/draft.htm'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get draft table\n",
    "            table = soup.find('table', {'id': 'drafts'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"college_link\":\n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                        elif td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ids.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_ID\", player_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dfs into a single df\n",
    "    draft_df = pd.concat(dfs, ignore_index=True)\n",
    "    draft_df.to_csv('nfl_draft_2000_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullDraftData(years_nfl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Pull NCAA team stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ncaa team offense data\n",
    "def pullTeamOffData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-team-offense.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get team offense table\n",
    "            table = soup.find('table', {'id': 'offense'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multilevel headers (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    team_offense_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    team_offense_df.to_csv('ncaaf_team_offense_1999_2019.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.sports-reference.com/cfb/years/1999-team-offense.html', ValueError('No tables found')]]\n"
     ]
    }
   ],
   "source": [
    "pullTeamOffData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ncaa team defense data\n",
    "def pullTeamDefData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-team-defense.html'\n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # get html\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get team defense table\n",
    "            table = soup.find('table', {'id': 'defense'})\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multilevel headers\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append dataframe to dataframes list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    team_defense_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    team_defense_df.to_csv('ncaaf_team_defense_1999_2019.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.sports-reference.com/cfb/years/1999-team-defense.html', ValueError('No tables found')]]\n"
     ]
    }
   ],
   "source": [
    "pullTeamDefData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NCAAF ratings\n",
    "def pullTeamRatings(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-ratings.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get ratings table\n",
    "            table = soup.find('table', {'id': 'ratings'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multilevel headers (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    team_ratings_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    team_ratings_df.to_csv('ncaaf_team_ratings_1999_2019.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullTeamRatings(years_ncaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Pull NFL team stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nfl team stats\n",
    "def pullNFLTeamStats(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.pro-football-reference.com/years/' + year + '/'\n",
    "            # get html\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get NFL team stats\n",
    "            table = soup.find_all('table', {'id': ['AFC', 'NFC']})\n",
    "            # read table as dataframe\n",
    "            tables = pd.read_html(str(table))\n",
    "            \n",
    "            # concatenate tables into single dataframe\n",
    "            df = pd.concat(tables, ignore_index=True)\n",
    "            \n",
    "            # remove division rows\n",
    "            df = df[~df['Tm'].isin(divisions)]\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append dataframe to dataframes list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    nfl_team_stats_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # save dataframe to csv\n",
    "    nfl_team_stats_df.to_csv('nfl_team_stats_1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roland\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pullNFLTeamStats(years_ncaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Pull player stats (minimum 14 Att/G, 75% of school games played)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ncaa player data\n",
    "def pullPlayerData(years, tableID):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    # list of tables with multilevel headers\n",
    "    multi_list = ['passing','rushing','receiving']\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-' + tableID + '.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get table\n",
    "            table = soup.find('table', {'id': tableID})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            if tableID in multi_list:\n",
    "                df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    out_df = pd.concat(dfs, ignore_index=True)\n",
    "    out_df.to_csv('ncaa_player_' + tableID + '_stats_1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in player_tables:\n",
    "    pullPlayerData(years, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Pull Consensus All-Americans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull All-Americans\n",
    "def pullAllAmericans(years):\n",
    "\n",
    "    base_url = 'https://www.sports-reference.com'\n",
    "    lst = []\n",
    "    \n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '.html'\n",
    "            \n",
    "            # put random delays between get requests \n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            # get html\n",
    "            res = requests.get(url, headers = headers)\n",
    "            \n",
    "            # Work around comments\n",
    "            comm = re.compile(\"<!--|-->\")\n",
    "            soup = BeautifulSoup(comm.sub(\"\", res.text), 'lxml')\n",
    "            for row in soup.find_all('div', id = 'div_all_americans'):\n",
    "                for p in row.find_all('p'):\n",
    "                    for a in p.find_all('a',limit=1):\n",
    "                        line = [year, p.text.split(',')[0].replace('*',''), p.text.split(',')[1].replace(' ',''), p.text.split(',')[2].lstrip(), base_url + a['href']]\n",
    "                    \n",
    "                    lst.append(line)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # add list to dataframe\n",
    "    df=pd.DataFrame(lst,columns=['Year','Player','Pos','School','NCAA_Link'])\n",
    "    # save as csv\n",
    "    df.to_csv('ncaa_all_americans__1999_2019.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullAllAmericans(years_ncaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull player stats (minimum 14 Att/G, 75% of school games played) -- OBSOLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# get ncaa player passing data\n",
    "def pullPassingData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-passing.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get passing table\n",
    "            table = soup.find('table', {'id': 'passing'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    #print(len(ncaa_links))\n",
    "    #print(len(player_ncaa_ids))\n",
    "    #print(df.head())\n",
    "    passing_df = pd.concat(dfs, ignore_index=True)\n",
    "    passing_df.to_csv('ncaa_player_passing_stats_1999_2019.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#pullPassingData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# get ncaa player rushing data\n",
    "def pullRushingData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-rushing.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get rushing table\n",
    "            table = soup.find('table', {'id': 'rushing'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    #print(len(ncaa_links))\n",
    "    #print(len(player_ncaa_ids))\n",
    "    #print(df.head())\n",
    "    rushing_df = pd.concat(dfs, ignore_index=True)\n",
    "    rushing_df.to_csv('ncaa_player_rushing_stats_1999_2019.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#pullRushingData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# get ncaa player receiving data\n",
    "def pullReceivingData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-receiving.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get receiving table\n",
    "            table = soup.find('table', {'id': 'receiving'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    #print(len(ncaa_links))\n",
    "    #print(len(player_ncaa_ids))\n",
    "    #print(df.head())\n",
    "    receiving_df = pd.concat(dfs, ignore_index=True)\n",
    "    receiving_df.to_csv('ncaa_player_receiving_stats_1999_2019.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#pullReceivingData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# get ncaa player kicking data\n",
    "def pullKickingData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-kicking.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get kicking table\n",
    "            table = soup.find('table', {'id': 'kicking'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    #print(len(ncaa_links))\n",
    "    #print(len(player_ncaa_ids))\n",
    "    #print(df.head())\n",
    "    kicking_df = pd.concat(dfs, ignore_index=True)\n",
    "    kicking_df.to_csv('ncaa_player_kicking_stats_1999_2019.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.sports-reference.com/cfb/years/1999-kicking.html', ValueError('No tables found')]]\n"
     ]
    }
   ],
   "source": [
    "#pullKickingData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# get ncaa player punting data\n",
    "def pullPuntingData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-punting.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get punting table\n",
    "            table = soup.find('table', {'id': 'punting'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    #print(len(ncaa_links))\n",
    "    #print(len(player_ncaa_ids))\n",
    "    #print(df.head())\n",
    "    punting_df = pd.concat(dfs, ignore_index=True)\n",
    "    punting_df.to_csv('ncaa_player_punting_stats_1999_2019.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.sports-reference.com/cfb/years/1999-punting.html', ValueError('No tables found')]]\n"
     ]
    }
   ],
   "source": [
    "#pullPuntingData(years_ncaa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
